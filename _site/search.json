[
  {
    "objectID": "guide.html",
    "href": "guide.html",
    "title": "User Guide",
    "section": "",
    "text": "User Guide"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ISSS608-VAA-Project",
    "section": "",
    "text": "Loan Default Prediction Challenge\nLoan default prediction through data exploration, analysis and prediction using advanced R technologies\nGroup 5"
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introduction to Visual Analytics Project",
    "section": "",
    "text": "Problem\nLoan lending is one of the most important financial services. Loan default risk assessment is part of the loan lending process; it does impact the profitability of financial institutions when the loan receiver is unable to pay back on time. Customers’ ability to pay back and willingness to pay back are essential considerations in loan lending.\n\n\nChallenges\nBeside the difficulty and accuracy of customer loan default prediction in real words, more than often, imbalanced data collected are also have significant impact on customer loan default prediction, imbalanced data can resulted in poor performance with traditional predictive models and evaluation metrics that assume a balanced class distribution.\n\n\nSolutions\nIn this project, first, two types of loans 1) new and 2) repeating loans together with customer demographics and loan history are used in this study to analyze customers’ ability to pay and willingness to pay, and predictive models are provided to evaluate factors selected on compared customer loan default prediction."
  },
  {
    "objectID": "poster.html",
    "href": "poster.html",
    "title": "Poster",
    "section": "",
    "text": "Poster"
  },
  {
    "objectID": "proposal.html",
    "href": "proposal.html",
    "title": "Storyboard - Loan Default Prediction in Nigeria",
    "section": "",
    "text": "Loan lending offers substantial profits, but also carries the risk of customers failing to repay loans on time, known as loan default risk, which is a type of credit risk.\nTo manage uncertainty, lending institutions have established lending standards and created predictive models to better evaluate the likelihood of loan repayment through credit risk assessment of customers.\nOver the past thirty years, loan default risk assessment has progressed from traditional credit scoring systems to data modeling utilizing data analytics and machine learning techniques."
  },
  {
    "objectID": "proposal.html#exploratory-data-analysis-eda",
    "href": "proposal.html#exploratory-data-analysis-eda",
    "title": "Storyboard - Loan Default Prediction in Nigeria",
    "section": "Exploratory data analysis (EDA)",
    "text": "Exploratory data analysis (EDA)\nThis is the first part of the R-Shiny application where users can perform Exploratory Data Analysis on this dataset, with the more common tools such as Distribution Analysis , Deviation Analysis and Scatterplot.\nDistribution analysis is a statistical technique used to assess the symmetry and shape of a dataset. It involves examining the frequency distribution of a predictor variable to identify whether the data is normally distributed or skewed. If a variable is found to be highly skewed, this can impact the accuracy and reliability of statistical models, and may require data transformation or specialized modeling techniques.\nExample:\n\n\n\n\n\nAnother tool can be employed is Deviation Analysis (box plot). Deviation analysis, often performed through box plots, is a graphical technique used to examine the relationship between a categorical predictor variable and a continuous variable. It provides a visual representation of the distribution of the continuous variable within each category of the predictor, and can reveal differences in variability or central tendency between groups. This can aid in identifying potential correlations between the two predictors and inform subsequent statistical analyses.\nExample:\n\n\n\n\n\nAnother type of analysis between 2 continuous variables is a scatterplot.\nA scatterplot is a graphical tool used to visualize the relationship between two continuous variables. Each point on the plot represents a combination of values for the two variables, and the pattern of points can reveal the direction, strength, and form of the correlation between the variables. Scatterplots can aid in identifying potential associations between variables and provide insights into the nature of the relationship, which can inform subsequent analyses or hypotheses."
  },
  {
    "objectID": "proposal.html#deeper-analysis",
    "href": "proposal.html#deeper-analysis",
    "title": "Storyboard - Loan Default Prediction in Nigeria",
    "section": "Deeper analysis",
    "text": "Deeper analysis\nThis section provides deep dive analysis on variables relationship between each other, both heatmap plot and mosaic plot are provided for modeling the relationship between continuous variables or categorical variables.\n\nUsers can perform heatmap and mosaic plot on different variables to determine whether which variables that can be applied to the subsequent prediction models."
  },
  {
    "objectID": "proposal.html#loan-default-prediction-using-classification-algorithms",
    "href": "proposal.html#loan-default-prediction-using-classification-algorithms",
    "title": "Storyboard - Loan Default Prediction in Nigeria",
    "section": "Loan default prediction using classification algorithms",
    "text": "Loan default prediction using classification algorithms\nUpon analyzed variables suitable for loan default prediction from section 1 and section 2, the section 3 here is provided interactive interface allow users to between different resampling methods.\n\n\n\nPrototype slide 3\n\n\nUsers can choose to include or exclude the highly correlated variables to see the effects of whether oversampling versus under sampling versus original data.\nUsers also can select different classification models, and our applications will showcase the results of the models using AOC analysis for prediction accuracy."
  },
  {
    "objectID": "rshiny.html",
    "href": "rshiny.html",
    "title": "RSHINY APP",
    "section": "",
    "text": "RSHINY APP"
  },
  {
    "objectID": "proposal.html#prototype-of-first-page",
    "href": "proposal.html#prototype-of-first-page",
    "title": "Storyboard - Loan Default Prediction in Nigeria",
    "section": "PROTOTYPE OF FIRST PAGE",
    "text": "PROTOTYPE OF FIRST PAGE\n\n\n\nPrototype slide 1"
  },
  {
    "objectID": "proposal.html#deeper-dive",
    "href": "proposal.html#deeper-dive",
    "title": "Storyboard - Loan Default Prediction in Nigeria",
    "section": "Deeper Dive",
    "text": "Deeper Dive\nThe second section of the R-shiny apps would provide users options to experiment with other analysis types such as Multi-collinearity test and Quasi Complete Separation.\n\nUsers can perform heatmap and mosaic plot on different variables to determine whether which variables that can be applied to the subsequent prediction models."
  },
  {
    "objectID": "proposal.html#multi--collinearity-curse",
    "href": "proposal.html#multi--collinearity-curse",
    "title": "Storyboard - Loan Default Prediction in Nigeria",
    "section": "Multi- Collinearity Curse",
    "text": "Multi- Collinearity Curse\nMulticollinearity refers to the situation in which two or more predictor variables in a machine learning model are highly correlated with each other, making it difficult to identify the independent effects of each variable on the dependent variable.\nIn a classification model, multicollinearity can lead to unstable and unreliable estimates of the coefficients and make it difficult to interpret the model’s results. It can also lead to overfitting and reduce the model’s predictive accuracy. To address multicollinearity, one can exclude the highly correlated variables when building classification model\nTo determine whether multiple variables are in danger of compromising the model by this issue, we can utilize a correlation matrix.\nA correlation matrix is a tool that can be used to identify multicollinearity by showing the pairwise correlations between each pair of predictor variables in the model. The correlation coefficient ranges from -1 to 1, where -1 indicates a perfect negative correlation (when one variable increases, the other decreases) and 1 indicates a perfect positive correlation (when one variable increases, the other increases).\nIn the context of multicollinearity, a high correlation coefficient (close to 1 or -1) between two predictor variables indicates a strong linear relationship between them. This means that one variable can be predicted well by the other, and it becomes difficult to disentangle the effects of the two variables on the dependent variable. It’s important to note that correlation doesn’t imply causation, and that multicollinearity can also occur between three or more predictor variables, not just pairs."
  },
  {
    "objectID": "proposal.html#section",
    "href": "proposal.html#section",
    "title": "Storyboard - Loan Default Prediction in Nigeria",
    "section": "",
    "text": "Quasi Complete Separation\nAnother issue that our Shiny Apps can allow our users to explore, is the problem with quasi complete separation. Quasi-complete separation is a situation that can occur in classification when a predictor variable perfectly separates the outcome variable into distinct categories.\nIn other words, when a predictor variable has a perfect linear relationship with the outcome variable, the logistic regression model can perfectly predict which category the outcome variable belongs to based on the value of the predictor variable. This means that the estimated coefficient for the predictor variable becomes infinitely large.\nQuasi-complete separation is particularly problematic in small or moderate-sized datasets because it can lead to overfitting and unreliable coefficient estimates. In addition, the model’s performance can be sensitive to small changes in the data, which can make it difficult to interpret the results.\nTo address quasi-complete separation, there are several approaches that can be taken. One approach is to remove the problematic predictor variable or combine it with other variables to reduce its impact.\nIn some cases, quasi-complete separation may be a real phenomenon in the data and may require a different approach altogether. For example, if the data has a natural threshold or cutoff point, such as in medical diagnosis or credit scoring, the logistic regression model may need to be modified to account for this threshold. Overall, it’s important to be aware of quasi-complete separation and to use appropriate methods to address it when it occurs.\nOne method of visual analytics to determine whether a predictor variable has the problem of quasi complete separation is by scatterplot. Quasi-complete separation can be visualized using a scatterplot, which can help to identify the problematic predictor variable and understand its relationship with the outcome variable.\nIn a scatterplot, the predictor variable is plotted on the x-axis and the outcome variable is plotted on the y-axis. When there is quasi-complete separation, we typically see that the data points fall into two distinct groups or clusters, with no overlap between the two groups."
  }
]